{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\26936\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\26936\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\26936\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\26936\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\26936\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\26936\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from textblob import TextBlob\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import textstat\n",
    "from collections import Counter\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def extract_features(text):\n",
    "    # Basic NLP processing\n",
    "    blob = TextBlob(text)\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    sentiment = sia.polarity_scores(text)\n",
    "    \n",
    "    # Tokenization\n",
    "    word_tokens = word_tokenize(text)\n",
    "    sentence_tokens = sent_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    non_stop_words = [word for word in word_tokens if word.lower() not in stop_words]\n",
    "\n",
    "    # Basic Text Features\n",
    "    num_words = len(word_tokens)\n",
    "    num_sentences = len(sentence_tokens)\n",
    "    num_unique_words = len(set(word_tokens))\n",
    "    num_chars = len(text)\n",
    "    num_non_stop_words = len(non_stop_words)\n",
    "    \n",
    "    # Lexical Diversity\n",
    "    lexical_diversity = num_unique_words / num_words if num_words > 0 else 0\n",
    "    \n",
    "    # Sentiment Scores\n",
    "    compound = sentiment['compound']\n",
    "    pos = sentiment['pos']\n",
    "    neu = sentiment['neu']\n",
    "    neg = sentiment['neg']\n",
    "    \n",
    "    # Readability Scores\n",
    "    flesch_reading_ease = textstat.flesch_reading_ease(text)\n",
    "    smog_index = textstat.smog_index(text)\n",
    "    \n",
    "    # Part-of-Speech Tags and Named Entity Recognition\n",
    "    pos_tags = blob.tags\n",
    "    noun_count = len([word for word, tag in pos_tags if tag.startswith('NN')])\n",
    "    verb_count = len([word for word, tag in pos_tags if tag.startswith('VB')])\n",
    "    adj_count = len([word for word, tag in pos_tags if tag.startswith('JJ')])\n",
    "    \n",
    "    # Syntactic Complexity\n",
    "    avg_sentence_length = num_words / num_sentences if num_sentences > 0 else 0\n",
    "    sentence_complexity = textstat.linsear_write_formula(text)\n",
    "    \n",
    "    # Word Frequency Distribution\n",
    "    freq_dist = FreqDist(non_stop_words)\n",
    "    most_common_words = freq_dist.most_common(5)\n",
    "    \n",
    "    # Initialize a list for common word counts with zeros\n",
    "    common_word_counts = [0] * 5  # Assuming you want the top 5 words\n",
    "    \n",
    "    # Fill in the actual counts for words that are present\n",
    "    for i, (word, count) in enumerate(most_common_words):\n",
    "        common_word_counts[i] = count\n",
    "    \n",
    "    return [\n",
    "        blob.sentiment.polarity, \n",
    "        blob.sentiment.subjectivity,\n",
    "        compound,\n",
    "        num_words,\n",
    "        num_sentences,\n",
    "        num_unique_words,\n",
    "        num_chars,\n",
    "        num_non_stop_words,\n",
    "        lexical_diversity,\n",
    "        pos,\n",
    "        neu,\n",
    "        neg,\n",
    "        noun_count,\n",
    "        verb_count,\n",
    "        adj_count,\n",
    "        flesch_reading_ease,\n",
    "        smog_index,\n",
    "        avg_sentence_length,\n",
    "        sentence_complexity\n",
    "    ] + common_word_counts  # Adding the most common word counts to the feature vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract Features 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Might fail due to torch version incompatible, consider a simpler version by calling 'extract_features' instead\n",
    "import spacy\n",
    "from transformers import pipeline\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.chunk import ne_chunk\n",
    "from nltk.util import ngrams\n",
    "from textblob import TextBlob\n",
    "import textstat\n",
    "from collections import Counter\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "# Load spaCy's English NLP model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Setup transformer pipeline for sentiment analysis\n",
    "# Might fail on some torch version !\n",
    "sentiment_analysis = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "def extract_combined_features(text):\n",
    "    # spaCy processing\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Transformer sentiment analysis\n",
    "    transformer_sentiment = sentiment_analysis(text)[0]\n",
    "\n",
    "    # TextBlob processing\n",
    "    blob = TextBlob(text)\n",
    "\n",
    "    # NLTK processing\n",
    "    word_tokens = word_tokenize(text)\n",
    "    sentence_tokens = sent_tokenize(text)\n",
    "    pos_tags = pos_tag(word_tokens)\n",
    "    named_entities = ne_chunk(pos_tags)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    non_stop_words = [word for word in word_tokens if word.lower() not in stop_words]\n",
    "    freq_dist = FreqDist(non_stop_words)\n",
    "\n",
    "    # NLTK Sentiment Analysis\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    nltk_sentiment = sia.polarity_scores(text)\n",
    "\n",
    "    # TextBlob Sentiment\n",
    "    tb_polarity = blob.sentiment.polarity\n",
    "    tb_subjectivity = blob.sentiment.subjectivity\n",
    "\n",
    "    # Basic metrics\n",
    "    num_tokens = len(doc)\n",
    "    num_sentences = len(list(doc.sents))\n",
    "    num_nouns = len([token for token in doc if token.pos_ == 'NOUN'])\n",
    "    num_verbs = len([token for token in doc if token.pos_ == 'VERB'])\n",
    "    num_adjectives = len([token for token in doc if token.pos_ == 'ADJ'])\n",
    "    num_entities = len(doc.ents)\n",
    "    entity_types = Counter(ent.label_ for ent in doc.ents)\n",
    "\n",
    "    # Dependency parsing metrics\n",
    "    root_verbs = len([token for token in doc if token.dep_ == 'ROOT' and token.pos_ == 'VERB'])\n",
    "\n",
    "    # Transformer sentiment scores\n",
    "    transformer_sentiment_label = transformer_sentiment['label']\n",
    "    transformer_sentiment_score = transformer_sentiment['score']\n",
    "\n",
    "    # Readability Scores\n",
    "    flesch_reading_ease = textstat.flesch_reading_ease(text)\n",
    "    smog_index = textstat.smog_index(text)\n",
    "    coleman_liau_index = textstat.coleman_liau_index(text)\n",
    "    automated_readability_index = textstat.automated_readability_index(text)\n",
    "\n",
    "    # NLTK Frequency Counts\n",
    "    pos_freq = Counter(tag for word, tag in pos_tags)\n",
    "\n",
    "    # Collecting features\n",
    "    features = [\n",
    "        tb_polarity, \n",
    "        tb_subjectivity,\n",
    "        nltk_sentiment['compound'],\n",
    "        len(word_tokens),\n",
    "        len(sentence_tokens),\n",
    "        len(set(word_tokens)),\n",
    "        len(text),\n",
    "        len(non_stop_words),\n",
    "        len(set(non_stop_words)) / len(word_tokens) if word_tokens else 0,\n",
    "        nltk_sentiment['pos'],\n",
    "        nltk_sentiment['neu'],\n",
    "        nltk_sentiment['neg'],\n",
    "        flesch_reading_ease,\n",
    "        smog_index,\n",
    "        coleman_liau_index,\n",
    "        automated_readability_index,\n",
    "        len(sentence_tokens) / len(word_tokens) if word_tokens else 0,\n",
    "        num_tokens,\n",
    "        num_sentences,\n",
    "        num_nouns,\n",
    "        num_verbs,\n",
    "        num_adjectives,\n",
    "        num_entities,\n",
    "        root_verbs,\n",
    "        transformer_sentiment_label,\n",
    "        transformer_sentiment_score\n",
    "    ]\n",
    "\n",
    "    # Adding NLTK entity counts and POS frequency\n",
    "    for entity_label in ['PERSON', 'ORGANIZATION', 'LOCATION']:\n",
    "        features.append(pos_freq.get(entity_label, 0))\n",
    "\n",
    "    # Adding spaCy entity type counts\n",
    "    for entity_type in ['PERSON', 'NORP', 'ORG', 'GPE', 'LOC', 'PRODUCT', 'EVENT', 'WORK_OF_ART', 'LAW', 'LANGUAGE', 'DATE', 'TIME', 'PERCENT', 'MONEY', 'QUANTITY', 'ORDINAL', 'CARDINAL']:\n",
    "        features.append(entity_types.get(entity_type, 0))\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Embedding, Input, concatenate, LSTM\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Load data\n",
    "train_data = pd.read_csv('..\\\\raw_data\\\\raw_data\\\\fulltrain.csv', header=None, names=['label', 'text'])\n",
    "test_data = pd.read_csv('..\\\\raw_data\\\\raw_data\\\\balancedtest.csv', header=None, names=['label', 'text'])\n",
    "\n",
    "X_texts = train_data['text'].values\n",
    "y = train_data['label'].values\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "y_categorical = to_categorical(y_encoded)\n",
    "\n",
    "# Tokenize text\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X_texts)\n",
    "\n",
    "# Set a reasonable max length for padding\n",
    "max_length = 1000  # Adjust based on your dataset distribution\n",
    "\n",
    "X_train_texts, X_val_texts, y_train, y_val = train_test_split(\n",
    "    X_texts, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Encode labels for both training and validation sets\n",
    "y_train_encoded = label_encoder.transform(y_train)\n",
    "y_val_encoded = label_encoder.transform(y_val)\n",
    "\n",
    "y_train_categorical = to_categorical(y_train_encoded)\n",
    "y_val_categorical = to_categorical(y_val_encoded)\n",
    "\n",
    "# Process both training and validation text data\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train_texts)\n",
    "X_val_seq = tokenizer.texts_to_sequences(X_val_texts)\n",
    "\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_length, truncating='post')\n",
    "X_val_pad = pad_sequences(X_val_seq, maxlen=max_length, truncating='post')\n",
    "\n",
    "# Simulate LIWC-like features\n",
    "# Might fail due to torch version incompatible, consider a simpler version by calling 'extract_features' instead\n",
    "additional_features_train = np.array([extract_combined_features(text) for text in X_train_texts])\n",
    "additional_features_val = np.array([extract_combined_features(text) for text in X_val_texts])\n",
    "\n",
    "# Feature Selection with RFE\n",
    "selector = RFE(RandomForestClassifier(n_estimators=100, random_state=42), n_features_to_select=5, step=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM (Single model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector.fit(additional_features_train, y_train_encoded)  # Fit RFE on the training set\n",
    "\n",
    "# Transform feature sets to include selected features\n",
    "additional_features_train_selected = selector.transform(additional_features_train)\n",
    "additional_features_val_selected = selector.transform(additional_features_val)\n",
    "\n",
    "# Fit and transform with MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "additional_features_train_scaled = scaler.fit_transform(additional_features_train_selected)\n",
    "additional_features_val_scaled = scaler.transform(additional_features_val_selected)\n",
    "\n",
    "# Define model architecture with Functional API to handle multiple inputs\n",
    "text_input = Input(shape=(max_length,), dtype='int32', name='text_input')\n",
    "additional_input = Input(shape=(5,), name='additional_input')\n",
    "\n",
    "# Text branch using LSTM\n",
    "embedded_text = Embedding(input_dim=5000, output_dim=100, input_length=max_length)(text_input)\n",
    "lstm_text = LSTM(64)(embedded_text)  # Using LSTM to process text\n",
    "\n",
    "# Combine branches\n",
    "combined = concatenate([lstm_text, additional_input])\n",
    "\n",
    "# Output layer\n",
    "predictions = Dense(units=len(label_encoder.classes_), activation='softmax')(combined)\n",
    "\n",
    "# Create model\n",
    "model = Model(inputs=[text_input, additional_input], outputs=predictions)\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Determine the number of unique classes\n",
    "num_classes = len(np.unique(y_train_encoded))\n",
    "\n",
    "# Create a dictionary for class weights\n",
    "class_weights = {i: 1.0 for i in range(num_classes)}\n",
    "class_weights[1] = 2.2  # Set the weight for the second class\n",
    "\n",
    "# Now, when you fit the model, include the class_weight argument\n",
    "model.fit(\n",
    "    [X_train_pad, additional_features_train_scaled], y_train_categorical,\n",
    "    epochs=2,\n",
    "    validation_data=([X_val_pad, additional_features_val_scaled], y_val_categorical),\n",
    "    batch_size=32,\n",
    "    class_weight=class_weights\n",
    ")\n",
    "\n",
    "# Model summary to see the architecture\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
