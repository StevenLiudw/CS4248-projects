{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "5eBcAxnku1je"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# execute this cell to generate Bugfree_fulltrain.csv from the base fulltrain.csv, which is used either as\n",
    "# direct input, or to generate the summarized articles for the transformer-based classifiers\n",
    "\n",
    "df = pd.read_csv('fulltrain.csv')\n",
    "\n",
    "df.iloc[:, 0] = pd.to_numeric(df.iloc[:, 0], errors='coerce')\n",
    "\n",
    "df = df[df.iloc[:, 0].notnull()]\n",
    "\n",
    "df.to_csv('Bugfree_fulltrain.csv', index=False)\n",
    "\n",
    "train_data = pd.read_csv('Bugfree_fulltrain.csv', header=None, names=['label', 'text'])\n",
    "test_data = pd.read_csv('balancedtest.csv', header=None, names=['label', 'text'])\n",
    "\n",
    "X_train = train_data['text']\n",
    "y_train = train_data['label']\n",
    "# minus all labels by one, so that range is now [0,3] for classification\n",
    "y_train -= 1\n",
    "X_test = test_data['text']\n",
    "y_test = test_data['label']\n",
    "y_test -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# load Google News pretrained word embeddings for sentence vectorization\n",
    "w2v_model = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# get the word embedding of a given sentence based on the pretrained word embeddings\n",
    "def get_word_embeddings(sentences):\n",
    "    split_sentences = [s.split() for s in sentences]\n",
    "    results = []\n",
    "    for i in range(len(split_sentences)):\n",
    "        words = split_sentences[i]\n",
    "        words_vecs = [w2v_model[word] for word in words if word in w2v_model]\n",
    "        if len(words_vecs) == 0:\n",
    "            results.append(np.zeros(300))\n",
    "        else:\n",
    "            words_vecs = np.array(words_vecs)\n",
    "            sent_vec = words_vecs.mean(axis=0)\n",
    "            results.append(sent_vec)\n",
    "            \n",
    "    return results\n",
    "\n",
    "def preprocess(sentences):\n",
    "    results = []\n",
    "    for s in sentences:\n",
    "        processed = re.sub(\"\\s+\", \" \", s)\n",
    "        # Remove leading and trailing whitespaces\n",
    "        processed = processed.strip()\n",
    "        results.append(processed)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def get_sentences(article):\n",
    "    doc = nlp(article)\n",
    "    # Remove leading and trailing whitespaces\n",
    "    sentences = [str(s).strip() for s in doc.sents]\n",
    "    return sentences\n",
    "\n",
    "def top_5_summarize(article):\n",
    "    k = 5\n",
    "    sentences = get_sentences(article)\n",
    "    if len(sentences) < k:\n",
    "        return article\n",
    "    # first preprocess\n",
    "    sentences_processed = preprocess(sentences)\n",
    "    # convert into word embedding vectors\n",
    "    embeddings = get_word_embeddings(sentences_processed)\n",
    "\n",
    "    # get pairwise cosine similarities\n",
    "    similarities = cosine_similarity(embeddings)\n",
    "    \n",
    "    # get sum of cosine similarities for each sentence with all other sentences\n",
    "    # e.g. index x stores for sentence x: sim(x, a) + sim(x, b) + sim(x, c) + ...\n",
    "    summed_similarities = np.sum(similarities, axis=1)\n",
    "\n",
    "    # get top k sentences with highest sum of cosine similarities\n",
    "    top_k_indices = np.argsort(summed_similarities)[-k:]\n",
    "    \n",
    "    # sort the top k indices to preserve ordering of sentences in the summary\n",
    "    sorted_top_k = np.sort(top_k_indices)\n",
    "\n",
    "    # extract non-preprocessed top k sentences and return as the summary\n",
    "    summary = [sentences[i] for i in sorted_top_k]\n",
    "    summary = \"\".join(summary)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate summarized datasets\n",
    "\n",
    "X_train_summ = X_train.apply(top_5_summarize)\n",
    "\n",
    "X_test_summ = X_test.apply(top_5_summarize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save summarized datasets with corresponding class labels to csv file\n",
    "\n",
    "merged_train_summ = pd.concat([X_train_summ, y_train], axis=1)\n",
    "merged_train_summ.columns = ['text', 'label']\n",
    "\n",
    "merged_test_summ = pd.concat([X_test_summ, y_test], axis=1)\n",
    "merged_test_summ.columns = ['text', 'label']\n",
    "\n",
    "merged_train_summ.to_csv('merged_summarized_fulltrain.csv', index=False)\n",
    "merged_test_summ.to_csv('merged_summarized_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
